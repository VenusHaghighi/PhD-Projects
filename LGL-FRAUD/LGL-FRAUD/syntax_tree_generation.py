import dgl
import torch
import torch.nn.functional as F
import numpy
import argparse
import time
from data_preparation import *
from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, precision_score, confusion_matrix
from BWGNN import *
from sklearn.model_selection import train_test_split
import json





def discretize_by_threshold(embeddings, threshold=0.5):
    """
    Convert [benign_prob, fraud_prob] vectors into symbolic labels.
    """
    fraud_probs = embeddings[:, 1]  # fraud probability
    labels = ["Likely Fraudulent" if prob >= threshold else "Likely Benign"
              for prob in fraud_probs]
    return labels





# Function to build the tree for a single node
def build_tree_for_one_node(node_id, graph, symbolic_embeddings, train_labels_str, relation_types):
    tree = {
        "center_id": node_id,
        "Feature": {
            "center": symbolic_embeddings[node_id],
            "1hop": {rel: {} for rel in relation_types}
        },
        "Label": {rel: {} for rel in relation_types}
    }

    for rel in relation_types:
        if rel in graph.etypes:
            dst = graph.successors(node_id, etype=rel).tolist()
            for nbr in dst:
                tree["Feature"]["1hop"][rel][nbr] = symbolic_embeddings[nbr]
                if nbr in train_labels_str:
                    tree["Label"][rel][nbr] = train_labels_str[nbr]

    return tree





if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='LLM_Augmented Fraud Detection Model')
    parser.add_argument('--dataset', type=str, default='yelp',
                        help='Dataset name, [amazon, yelp]')
    parser.add_argument("--train_ratio", type=float, default=0.4, help="Training ratio")
    parser.add_argument("--hid_dim", type=int, default=64, help="Hidden layer dimension")
    parser.add_argument("--order", type=int, default=2, help="Order C in Beta Wavelet")
    parser.add_argument("--homo", type=int, default=0, help="1 for BWGNN(Homo) and 0 for BWGNN(Hetero)")
    parser.add_argument("--epoch", type=int, default=500, help="The max number of epochs")
    parser.add_argument("--run", type=int, default=1, help="Running times")
    
    
    

    parser.add_argument('--train_size', type=float, default=0.4,
                        help='Train size of nodes.')
    parser.add_argument('--val_size', type=float, default=0.1,
                        help='Val size of nodes.')
    parser.add_argument('--seed', type=int, default=717,
                        help='Collecting neighbots in n hops.')

    parser.add_argument('--norm_feat', action='store_true', default=False,
                        help='Using group norm, default False')
    parser.add_argument('--grp_norm', action='store_true', default=False,
                        help='Using group norm, default False')
    parser.add_argument('--force_reload', action='store_true', default=False,
                        help='Using group norm, default False')
    parser.add_argument('--add_self_loop', action='store_true', default=False,
                    help='add self-loop to all the nodes')

    parser.add_argument('--base_dir', type=str, default='~/.dgl',
                        help='Directory for loading graph data.')
    
    
    args = vars(parser.parse_args())
    print(args)
    
    #load the data
    data, graph = prepare_data(args)
    print(data)
    
    # Convert training node IDs to a set for fast lookup
    train_nid_set = set(data.train_nid.tolist())

    # Build train label dictionary {node_id: 'Fraudulent' / 'Benign'}
    train_labels_str = {
        int(nid): "Fraudulent" if data.labels[nid].item() == 1 else "Benign"
        for nid in data.train_nid
    }
    
    
    # Define relation types
    relation_types = graph.etypes
    
    # load final_embeddings generated by BWGNN
    final_embeddings = torch.load("final_embedding.pt").detach()
    
    
    # Discretization phase
    symbolic_embeddings = discretize_by_threshold(final_embeddings)
    print("symbolic labels:", symbolic_embeddings)
    
    # Generate trees for all test nodes
    test_nodes = data.test_nid.tolist()
    syntax_trees = []
    
    print("Generating Graph-Syntax-Tree starts...")
    for test_node in test_nodes:
        tree = build_tree_for_one_node(
            node_id=test_node,
            graph=graph,
            symbolic_embeddings=symbolic_embeddings,
            train_labels_str=train_labels_str,
            relation_types=relation_types
        )
        syntax_trees.append(tree)
    
    
    with open("syntax_trees.json", "w") as f:
        json.dump(syntax_trees, f, indent=2)
    
    
    